#!/bin/bash
#SBATCH --job-name=mvae_run_gpu
#SBATCH --output=vae_run.out
#SBATCH --error=vae_run.err
#SBATCH -A es_chatzi
#SBATCH --gpus=v100:2
#SBATCH --cpus-per-task=4
#SBATCH --mem-per-cpu=16G
#SBATCH --time=18:00:00
#SBATCH --mail-type=END,FAIL

echo "üîÑ Job started on $(hostname) at $(date)"
echo "üß† Using $SLURM_CPUS_ON_NODE CPU cores"

# ‚úÖ Load modules with CUDA & TensorFlow support
module load stack/2024-06
module load gcc/12.2.0
module load python_cuda/3.11.6

# ‚úÖ Set CUDA environment (required for XLA)
export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CUDA_EULER_ROOT
export CUDA_DIR=$CUDA_EULER_ROOT

# ‚úÖ (Optional) Confirm GPU availability from within job
python -c "import tensorflow as tf; print('üü¢ TF sees GPU:', tf.config.list_physical_devices('GPU'))"

echo "üîç Checking GPU device..."
nvidia-smi

# ‚úÖ Run your model
python -u scripts/vae_generator.py

echo "‚úÖ Job finished at $(date)"

